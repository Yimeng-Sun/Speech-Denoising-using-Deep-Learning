{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qQvOfdVRMB1w"
   },
   "source": [
    "\n",
    "#### Speech Denoising using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "FwHUVe6AiLbT",
    "outputId": "935e4182-32ca-4678-b491-6bce0e047185"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2c6JyjQWhlWO"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "diGKtnQSi03H"
   },
   "outputs": [],
   "source": [
    "def filegen(path,picklepath):\n",
    "  train = []\n",
    "  for f in sorted(glob.glob(path)):\n",
    "    s, sr=librosa.load(f, sr=None)\n",
    "    S=librosa.stft(s, n_fft=1024, hop_length=512)\n",
    "    train.append(np.abs(S).T)\n",
    "  with open(picklepath, 'wb') as fi:\n",
    "    pickle.dump(train,fi)\n",
    "  return(train)\n",
    "\n",
    "\n",
    "def validfilegen(path,picklepath):\n",
    "  valid = []\n",
    "  if(path == '/content/drive/My Drive/homework3/timit-homework/v/vx*'):\n",
    "    for f1 in sorted(glob.glob(path)):\n",
    "      s, sr=librosa.load(f1, sr=None)\n",
    "      S=librosa.stft(s, n_fft=1024, hop_length=512)\n",
    "      valid.append((S, len(s)))\n",
    "  else:\n",
    "    for f2 in sorted(glob.glob(path)):\n",
    "      s, sr=librosa.load(f2, sr=None)\n",
    "      S=librosa.stft(s, n_fft=1024, hop_length=512)\n",
    "      valid.append((S, s))\n",
    "  with open(picklepath, 'wb') as fi:\n",
    "    pickle.dump(valid,fi)\n",
    "  return(valid)\n",
    "\n",
    "def testfilegen(path,picklepath):\n",
    "  test = []\n",
    "  for f3 in sorted(glob.glob(path)):\n",
    "    s, sr=librosa.load(f3, sr=None)\n",
    "    S=librosa.stft(s, n_fft=1024, hop_length=512)\n",
    "    test.append((S, len(s),sr))\n",
    "  with open(picklepath, 'wb') as fi:\n",
    "    pickle.dump(test,fi)\n",
    "  return(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mp8R6DxH8SIE"
   },
   "source": [
    "####START *********************This portion for the loading of files takes 2 hours of time********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMUwJpwPj6-8"
   },
   "outputs": [],
   "source": [
    "#Loading the training data\n",
    "train_dirty = filegen('/content/drive/My Drive/homework3/timit-homework/tr/trx*','/content/drive/My Drive/homework3/train_dirty.pkl')\n",
    "train_clean = filegen('/content/drive/My Drive/homework3/timit-homework/tr/trs*','/content/drive/My Drive/homework3/train_clean.pkl')\n",
    "train_noise = filegen('/content/drive/My Drive/homework3/timit-homework/tr/trn*','/content/drive/My Drive/homework3/train_noise.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fhn0Kp8ikSo1"
   },
   "outputs": [],
   "source": [
    "#Loading the validation data\n",
    "valid_dirty = filegen('/content/drive/My Drive/homework3/timit-homework/v/vx*','/content/drive/My Drive/homework3/valid_dirty.pkl')\n",
    "valid_clean = filegen('/content/drive/My Drive/homework3/timit-homework/v/vs*','/content/drive/My Drive/homework3/valid_clean.pkl')\n",
    "valid_noise = filegen('/content/drive/My Drive/homework3/timit-homework/v/vn*','/content/drive/My Drive/homework3/valid_noise.pkl')\n",
    "\n",
    "valid_dirty_main = validfilegen('/content/drive/My Drive/homework3/timit-homework/v/vx*','/content/drive/My Drive/homework3/valid_dirty_main.pkl')\n",
    "valid_clean_main = validfilegen('/content/drive/My Drive/homework3/timit-homework/v/vs*','/content/drive/My Drive/homework3/valid_clean_main.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJlRwfQAkU2q"
   },
   "outputs": [],
   "source": [
    "#Loading the test data\n",
    "test_dirty = filegen('/content/drive/My Drive/homework3/timit-homework/te/tex*','/content/drive/My Drive/homework3/test_dirty.pkl')\n",
    "test_dirty_main = testfilegen('/content/drive/My Drive/homework3/timit-homework/te/tex*','/content/drive/My Drive/homework3/test_dirty_main.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Az287Uy28bVK"
   },
   "source": [
    "####END *********************This portion for the loading of files takes 2 hours of time********************************\n",
    "\n",
    "Instead use the below portion!\n",
    "I have added the pickled files to the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8bSzKVvcAeYZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_pickle(path):\n",
    "  with open(path, 'rb') as fi:\n",
    "    fil = pickle.load(fi)\n",
    "  return fil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8h5gx0mFA3s-"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Loading the pickled training data\n",
    "train_clean = load_pickle('/content/drive/My Drive/homework3/train_clean.pkl')\n",
    "train_dirty = load_pickle('/content/drive/My Drive/homework3/train_dirty.pkl')\n",
    "train_noise = load_pickle('/content/drive/My Drive/homework3/train_noise.pkl')\n",
    "\n",
    "#Loading the pickled validation data\n",
    "valid_clean = load_pickle('/content/drive/My Drive/homework3/valid_clean.pkl')\n",
    "valid_dirty = load_pickle('/content/drive/My Drive/homework3/valid_dirty.pkl')\n",
    "valid_noise = load_pickle('/content/drive/My Drive/homework3/valid_noise.pkl')\n",
    "valid_dirty_main = load_pickle('/content/drive/My Drive/homework3/valid_dirty_main.pkl')\n",
    "valid_clean_main = load_pickle('/content/drive/My Drive/homework3/valid_clean_main.pkl')\n",
    "\n",
    "#Loading the pickled test data\n",
    "test_dirty = load_pickle('/content/drive/My Drive/homework3/test_dirty.pkl')\n",
    "test_dirty_main = load_pickle('/content/drive/My Drive/homework3/test_dirty_main.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hcWliF0i-wn-"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calc_IBM(signal,noise):\n",
    "  ibm=np.greater(signal,noise)*1\n",
    "  return(ibm)\n",
    "\n",
    "def calc_snr(x,s):\n",
    "  n = np.sum(np.square(x))\n",
    "  d = np.sum(np.square(x-s))\n",
    "  snr = 10* np.log10(n/d)\n",
    "  return(snr)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fMa1RyCTvT3"
   },
   "source": [
    "The Ideal Binary Masks are being calculated here using the defined function, calc_IBM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JiX7_AcigRro"
   },
   "outputs": [],
   "source": [
    "M = []\n",
    "for i in range(1200):\n",
    "  M.append(calc_IBM(train_clean[i],train_noise[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "Tajo7DGD7d_X",
    "outputId": "c7b8a907-187c-4128-deba-f9a77e2ecb7a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ViXD_nLO7U2p"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [ None,None,np.shape(train_clean[0])[1]])\n",
    "y = tf.placeholder(tf.float32, [None,None,np.shape(train_clean[0])[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RB-mBLySTVOF"
   },
   "source": [
    "#### LSTM Model Description:\n",
    "An LSTM cell with 513 units, having a dropout wrapper with dropout level(0.9) added for each RNN unit output.\n",
    "At last an output dense layer with 513 units and a sigmoid activation is implemented.\n",
    "Dynamic RNN to create a recurrent neural network, dynamically construct the graph when it is executed.\n",
    "He initialization is used in the in the initial mentioned LSTM cell and in the output dense layer using variance_scaling_initializer module.\n",
    "Mean Squared Error loss function and Adam optimizer are used with learning rate of 0.001.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "id": "fPfbVlDs6TX7",
    "outputId": "62e345b6-dcbc-4aa8-9b78-8c9f36b5ddfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-12-b262e18d3b46>:2: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-12-b262e18d3b46>:4: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-12-b262e18d3b46>:5: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "#LSTM Model:\n",
    "\n",
    "cell_lstm = tf.contrib.rnn.LSTMCell(513, initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "dropout_lstm_layer = tf.nn.rnn_cell.DropoutWrapper(cell_lstm , output_keep_prob=0.9)\n",
    "lstm_output_layer , state_lstm = tf.nn.dynamic_rnn(dropout_lstm_layer , x , dtype=tf.float32)\n",
    "output_layer = tf.layers.dense(lstm_output_layer , 513 , activation=tf.nn.sigmoid , kernel_initializer=tf.contrib.layers.variance_scaling_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "2UVMboHl7Jpl",
    "outputId": "7e53b576-5bbd-4b37-cc2e-194f505b4d37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "cost =tf.losses.mean_squared_error(labels=y, predictions=output_layer)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HeG3wOVf7mqZ"
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "colab_type": "code",
    "id": "Mz1ns_Bt8FFb",
    "outputId": "d29d31db-9e68-4e24-c532-d347896ae2b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss 26.31024107336998\n",
      "Epoch 1 loss 22.25534576177597\n",
      "Epoch 2 loss 20.32280106842518\n",
      "Epoch 3 loss 19.207242481410503\n",
      "Epoch 4 loss 18.507744327187538\n",
      "Epoch 5 loss 17.856200844049454\n",
      "Epoch 6 loss 17.460377991199493\n",
      "Epoch 7 loss 17.039649829268456\n",
      "Epoch 8 loss 16.640930481255054\n",
      "Epoch 9 loss 16.3269679620862\n",
      "Epoch 10 loss 16.103137105703354\n",
      "Epoch 11 loss 15.803884841501713\n",
      "Epoch 12 loss 15.562882408499718\n",
      "Epoch 13 loss 15.254823133349419\n",
      "Epoch 14 loss 15.207882583141327\n",
      "Epoch 15 loss 15.170404076576233\n",
      "Epoch 16 loss 14.980057381093502\n",
      "Epoch 17 loss 14.64017367362976\n",
      "Epoch 18 loss 14.705688059329987\n",
      "Epoch 19 loss 14.369383201003075\n",
      "Epoch 20 loss 14.150674395263195\n",
      "Epoch 21 loss 13.971485584974289\n",
      "Epoch 22 loss 13.811717718839645\n",
      "Epoch 23 loss 13.621875204145908\n",
      "Epoch 24 loss 13.504915989935398\n",
      "Epoch 25 loss 13.386620827019215\n",
      "Epoch 26 loss 13.297039404511452\n",
      "Epoch 27 loss 13.167198121547699\n",
      "Epoch 28 loss 13.120986074209213\n",
      "Epoch 29 loss 12.945222370326519\n",
      "Epoch 30 loss 12.998136013746262\n",
      "Epoch 31 loss 12.873894765973091\n",
      "Epoch 32 loss 12.724379979074001\n",
      "Epoch 33 loss 12.68614586442709\n",
      "Epoch 34 loss 12.619892433285713\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 35\n",
    "n_batch = 10\n",
    "for epoch in range(n_epochs):\n",
    "  epoch_loss = 0\n",
    "  for i in range(0,1200,n_batch):\n",
    "    epoch_x,epoch_y = np.array(train_dirty[i:i+n_batch]).reshape(n_batch,-1,513),np.array(M[i:i+n_batch]).reshape(n_batch,-1,513)\n",
    "    _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "    epoch_loss += c\n",
    "  print(\"Epoch\",epoch,\"loss\",epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PKjFIybfcSaO"
   },
   "outputs": [],
   "source": [
    "\n",
    "valid_clean_pred = []\n",
    "for i in range(1200):\n",
    "  M_val =  sess.run(output_layer, {x: valid_dirty[i].reshape(-1, valid_dirty[i].shape[0], 513)})\n",
    "  M_val = np.multiply(M_val, valid_dirty[i])\n",
    "  M_val = M_val.reshape(valid_dirty[i].shape[0], 513)\n",
    "  hadamard_product = np.multiply(np.divide(valid_dirty_main[i][0], np.abs(valid_dirty_main[i][0])), M_val.T)\n",
    "  pred_val = librosa.istft(hadamard_product, hop_length = 512, length = valid_dirty_main[i][1])\n",
    "  valid_clean_pred.append(pred_val)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q7pIIPKbvc6a"
   },
   "outputs": [],
   "source": [
    "\n",
    "validation_snr = []\n",
    "snr_val = 0\n",
    "for i in range(len(valid_clean_main)):\n",
    "  validation_snr.append(calc_snr(valid_clean_main[i][1], valid_clean_pred[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "jLlkDYaibZjD",
    "outputId": "61e6318f-0bc3-404c-8c25-1bd0a5bb62f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The least SNR value in the validation set is:  2.4491603672504425\n",
      "The mean SNR value in the validation set is:  10.569707577551405\n",
      "The greatest SNR value in the validation set is:  24.29755449295044\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"The least SNR value in the validation set is: \",min(validation_snr))\n",
    "print(\"The mean SNR value in the validation set is: \",np.mean(validation_snr))\n",
    "print(\"The greatest SNR value in the validation set is: \",max(validation_snr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bzWxVZ5ZZ4E8"
   },
   "source": [
    "The validation results, came out be having a <br>mean SNR value across the predictions by the LSTM network for the dirty validation signals to be 10.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K7QXfzRg5JKh"
   },
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import  files\n",
    "test_clean_pred = []\n",
    "for i in range(len(test_dirty)):\n",
    "  M_test =  sess.run(output_layer, {x: test_dirty[i].reshape(-1, test_dirty[i].shape[0], 513)})\n",
    "  M_test = np.multiply(M_test, test_dirty[i])\n",
    "  M_test = M_test.reshape(test_dirty[i].shape[0], 513)\n",
    "  hadamard_product = np.multiply(np.divide(test_dirty_main[i][0], np.abs(test_dirty_main[i][0])), M_test.T) \n",
    "  pred_test = librosa.istft(hadamard_product, hop_length = 512, length = test_dirty_main[i][1])\n",
    "  librosa.output.write_wav('test_recon_'+str(i)+'.wav', pred_test, test_dirty_main[i][2])\n",
    "  test_clean_pred.append(pred_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ax6ZByFCp6fC",
    "outputId": "519657bb-36c5-4cc7-91b6-23773e48ea94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_clean_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RACe7PW9i6NC"
   },
   "source": [
    "The test clean reconstructed sounds are generated by the above code and the sound files would be downloaded by the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WVgkCDqhDdzv",
    "outputId": "5c2a701d-8818-49df-fd89-9bf7994aaf82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test reconstructed files generated and downloaded\n"
     ]
    }
   ],
   "source": [
    " \n",
    " for i in range(len(test_clean_pred)):\n",
    "   files.download('test_recon_' + str(i) + '.wav')  \n",
    "   \n",
    "print(\"Test reconstructed files generated and downloaded\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h9C8-d95oxTQ"
   },
   "source": [
    "The test files are downloaded and submitted as a zip folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ko4vkLG2oTSx"
   },
   "source": [
    "The above code uses the colab imported files it sometimes fails to execute generating a Failed to Fetch exception, just need to re-run the cell.\n",
    "A reported bug,\n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/55713290/typeerror-failed-to-fetch-google-colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rhXH8eInlA_f"
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "for i in range(len(test_clean_pred)):\n",
    "  ipd.Audio('test_recon_'+str(i)+'.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bw7SAY2x9uoE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Deep_Learning_HW3_MNIST_SDN_Prahasan_Gadugu.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
